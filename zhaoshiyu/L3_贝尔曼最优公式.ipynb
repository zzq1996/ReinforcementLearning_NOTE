{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 第3课：贝尔曼最优公式（Bellman Optimality Equation）\n",
    "\n",
    "强化学习的最终目标是：**学习一个最优策略（optimal policies）** $\\pi_{*}$，使得智能体在任意状态下都能获得最大长期回报。\n",
    "要实现这一目标，我们从策略改进的思想出发，逐步构建出**最优策略**和对应的**最优值函数(optimal state value)**，并推导出贝尔曼最优公式。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：例子 - 如何改进策略\n",
    "\n",
    "我们回顾上节中介绍的动作值函数 $Q_{\\pi}(s, a)$：表示在状态 $s$ 下采取动作 $a$，并之后遵循策略 $\\pi$ 所获得的期望回报。\n",
    "\n",
    "那么，如果我们发现某个动作 $a'$ 的 $Q_{\\pi}(s, a')$ 明显大于其他动作，我们就可以考虑在状态 $s$ 下**改变策略，选取更优动作**。\n",
    "\n",
    "### 策略改进原则 —— select the action with the greatest action value：\n",
    "\n",
    "> 对所有状态 $s$，若我们总是选择动作 $a = \\arg\\max_{a} Q_{\\pi}(s, a)$，则新策略比旧策略至少不差，通常更优。\n",
    "\n",
    "这种“选择最大动作”的策略称为 **贪婪策略（greedy policy）**：\n",
    "\n",
    "$$\n",
    "\\pi'(s) = \\arg\\max_a Q_{\\pi}(s,a)\n",
    "$$\n",
    "\n",
    "这个思想是策略迭代、值迭代等方法的基础。\n",
    "\n"
   ],
   "id": "79fcdc4fe0ea0683"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2：最优策略和公式推导\n",
    "\n",
    "### 2.1 最优策略(optimal policy)定义：\n",
    "\n",
    "存在至少一个策略 $\\pi^*$，使得在所有状态 $s$ 下，状态值最大：\n",
    "\n",
    "$$\n",
    "V_{\\pi_*}(s) = \\max_\\pi V_\\pi(s), \\quad \\forall s\n",
    "$$\n",
    "\n",
    "其对应的最优值函数（optimal value function）定义如下：\n",
    "\n",
    "* **最优状态值函数：**\n",
    "\n",
    "  $$\n",
    "  V_*(s) = \\max_\\pi V_\\pi(s)\n",
    "  $$\n",
    "\n",
    "* **最优动作值函数：**\n",
    "\n",
    "  $$\n",
    "  Q_*(s, a) = \\max_\\pi Q_\\pi(s, a)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 最优贝尔曼方程（Bellman Optimality Equation, BOE）\n",
    "\n",
    "我们现在不再针对具体策略 $\\pi$，而是考虑“最优可能的动作”。这就导致贝尔曼公式中的期望转化为最大值。\n",
    "\n",
    "#### 状态值函数形式：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "V_*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s,a) V_*(s') \\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "#### 动作值函数形式：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "Q_*(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q_*(s', a')\n",
    "}\n",
    "$$\n",
    "\n",
    "这些递归公式就称为**贝尔曼最优方程（Bellman Optimality Equations）**。\n",
    "\n",
    "它是**非线性的**（包含了 max），但定义了最优策略应满足的条件。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 2.3 Matrix-vector Form of BOE\n",
    "\n",
    "我们希望将上述两个贝尔曼方程写成适合数值计算的向量/矩阵形式。\n",
    "\n",
    "---\n",
    "\n",
    "#### 状态值函数的矩阵形式\n",
    "\n",
    "定义：\n",
    "\n",
    "* 状态数为 $|S| = n$，动作数为 $|A| = m$；\n",
    "* $\\mathbf{v}_* \\in \\mathbb{R}^{n \\times 1}$：最优状态值向量；\n",
    "* $\\mathbf{R}^a \\in \\mathbb{R}^{n \\times 1}$：在动作 $a$ 下每个状态的即时奖励；\n",
    "* $\\mathbf{P}^a \\in \\mathbb{R}^{n \\times n}$：在动作 $a$ 下的状态转移矩阵；\n",
    "* $\\gamma \\in [0,1)$：折扣因子。\n",
    "\n",
    "则最优贝尔曼方程变为：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbf{v}_* = \\max_{a \\in \\mathcal{A}} \\left[ \\mathbf{R}^a + \\gamma \\mathbf{P}^a \\mathbf{v}_* \\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "其中 $\\max$ 操作是逐元素的，对每个状态 $s$，从所有动作中选最大值。\n",
    "\n",
    "---\n",
    "\n",
    "#### 动作值函数的矩阵形式\n",
    "\n",
    "定义：\n",
    "\n",
    "* $\\mathbf{Q}_* \\in \\mathbb{R}^{n \\times m}$：最优动作值函数矩阵（每行是一个状态，每列是一个动作）；\n",
    "* 对于每个动作 $a$，定义：\n",
    "\n",
    "  * $\\mathbf{R}^a \\in \\mathbb{R}^{n \\times 1}$：每个状态在动作 $a$ 下的奖励；\n",
    "  * $\\mathbf{P}^a \\in \\mathbb{R}^{n \\times n}$：动作 $a$ 的转移概率矩阵。\n",
    "\n",
    "则 BOE 可写为：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbf{Q}_*[:, a] = \\mathbf{R}^a + \\gamma \\mathbf{P}^a \\cdot \\max_{a'} \\mathbf{Q}_*[:, a']\n",
    "}\n",
    "\\quad \\text{for all } a \\in \\mathcal{A}\n",
    "$$\n",
    "\n",
    "这表示每一列（即每个动作）的 $Q$-值依赖于下一个状态的最大 $Q$-值。\n",
    "\n",
    "---\n",
    "\n",
    "#### 实践意义\n",
    "\n",
    "| 形式    | 数学形式                                                                                          | 应用                    |\n",
    "| ----- | --------------------------------------------------------------------------------------------- | --------------------- |\n",
    "| 状态值形式 | $\\mathbf{v}_* = \\max_a \\left[ \\mathbf{R}^a + \\gamma \\mathbf{P}^a \\mathbf{v}_* \\right]$        | 用于值迭代、策略评估            |\n",
    "| 动作值形式 | $\\mathbf{Q}_*[:, a] = \\mathbf{R}^a + \\gamma \\mathbf{P}^a \\cdot \\max_{a'} \\mathbf{Q}_*[:, a']$ | 用于 Q-learning、深度强化学习等 |\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 小结\n",
    "\n",
    "* 状态值形式偏向于策略评估（如策略迭代中的 Policy Evaluation）；\n",
    "* 动作值形式则是**无策略策略改进**的基础，如 Q-learning、DQN；\n",
    "* 二者都可转化为矩阵运算，可用于数值解法和模型计算；\n",
    "* 所有最优策略都满足这些方程，因此**求解 BOE 就是求解最优策略的核心问题**。\n"
   ],
   "id": "803edd47ea8afb02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 3：公式求解与最优性\n",
    "\n",
    "由于贝尔曼最优方程是**非线性方程组**，直接求解很困难，特别在状态空间较大时（例如图像、文本等）。\n",
    "\n",
    "### 3.0 Contraction Mapping Theorem（压缩映射定理）\n",
    "\n",
    "当然可以，Contraction Mapping Theorem（压缩映射定理）/ fixedpoint theorem是强化学习理论中的一个重要数学基础，尤其在证明值迭代（Value Iteration）收敛性时扮演关键角色。我们来一步步解释：\n",
    "\n",
    "---\n",
    "\n",
    "#### 一句话理解\n",
    "\n",
    "> 如果一个映射会“压缩”距离（即让两个输入之间的差变小），那么不断应用这个映射会收敛到唯一的固定点。\n",
    "\n",
    "---\n",
    "\n",
    "#### 数学定义（压缩映射）\n",
    "\n",
    "设 $(X, d)$ 是一个**完备度量空间**，也就是说：\n",
    "\n",
    "* $X$ 是一个集合；\n",
    "* $d(x, y)$ 是定义在 $X$ 上的距离函数（满足常规距离性质）；\n",
    "* “完备”表示这个空间中每个柯西序列都收敛。\n",
    "\n",
    "一个映射 $T: X \\rightarrow X$ 被称为**压缩映射**（Contraction mapping），如果存在一个常数 $0 \\leq \\beta < 1$，使得对任意 $x, y \\in X$ 都满足：\n",
    "\n",
    "$$\n",
    "d(T(x), T(y)) \\leq \\beta \\cdot d(x, y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Contraction Mapping Theorem（Banach Fixed Point Theorem）\n",
    "\n",
    "**定理内容：**\n",
    "\n",
    "如果 $T: X \\rightarrow X$ 是一个压缩映射，且 $X$ 是完备度量空间，则：\n",
    "\n",
    "1. **存在唯一固定点** $x^* \\in X$，使得 $T(x^*) = x^*$；\n",
    "2. 对任意初始点 $x_0 \\in X$，迭代序列 $x_{k+1} = T(x_k)$ 会收敛到这个唯一固定点 $x^*$；\n",
    "3. 收敛速度是**几何级数**的，满足：\n",
    "\n",
    "   $$\n",
    "   d(x_k, x^*) \\leq \\frac{\\beta^k}{1 - \\beta} d(x_1, x_0)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 在强化学习中的应用\n",
    "\n",
    "强化学习中的很多“值函数迭代算法”（比如值迭代、策略评估）都用到了这个定理。\n",
    "\n",
    "#### 关键例子：贝尔曼期望算子是压缩映射\n",
    "\n",
    "定义贝尔曼期望算子（Bellman Expectation Operator）：\n",
    "\n",
    "$$\n",
    "\\mathcal{T}^\\pi V(s) = \\mathbb{E}_\\pi \\left[ R(s, a) + \\gamma V(s') \\right]\n",
    "$$\n",
    "\n",
    "在 $\\|\\cdot\\|_\\infty$ 范数下，它满足：\n",
    "\n",
    "$$\n",
    "\\| \\mathcal{T}^\\pi V - \\mathcal{T}^\\pi U \\|_\\infty \\leq \\gamma \\cdot \\| V - U \\|_\\infty\n",
    "$$\n",
    "\n",
    "因为 $\\gamma \\in [0,1)$，所以 $\\mathcal{T}^\\pi$ 是压缩映射。\n",
    "\n",
    "✅ 由此推出：**对任意初始值函数 $V_0$**，不断应用贝尔曼期望算子：\n",
    "\n",
    "$$\n",
    "V_{k+1} = \\mathcal{T}^\\pi V_k\n",
    "$$\n",
    "\n",
    "将会收敛到唯一的固定点 $V^\\pi$，即策略 $\\pi$ 的值函数。\n",
    "\n",
    "---\n",
    "\n",
    "#### 总结\n",
    "\n",
    "| 项目        | 内容                                                     |             |   |\n",
    "| --------- | ------------------------------------------------------ | ----------- | - |\n",
    "| 压缩映射定义    | $d(T(x), T(y)) \\leq \\beta d(x, y)$ 且 $\\beta \\in [0,1)$ |             |   |\n",
    "| 压缩映射定理内容  | 有唯一固定点，迭代必收敛，且收敛快                                      |             |   |\n",
    "| 在强化学习中的应用 | 贝尔曼算子是压缩映射，值迭代/策略评估的收敛性依赖此定理                           |             |   |\n",
    "| 所用距离/空间   | 常使用 ( \\|V - U\\|\\_\\infty = \\max\\_s                      | V(s) - U(s) | ) |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.1 值迭代（Value Iteration）—— 基于压缩映射定理\n",
    "\n",
    "基于 Bellman 最优方程的**迭代逼近算法**，从任意初始 $V_0(s)$ 开始：\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V_k(s') \\right]\n",
    "$$\n",
    "\n",
    "不断更新，直到 $V_k$ 收敛为 $V^*$。之后通过：\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "得到最优策略。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 策略迭代（Policy Iteration）\n",
    "\n",
    "交替执行两个步骤：\n",
    "\n",
    "* **策略评估**：固定策略 $\\pi$，求 $V^\\pi(s)$\n",
    "* **策略改进**：使用 $Q^\\pi(s,a)$，得到新的 $\\pi'$\n",
    "\n",
    "这两个算法将在下一课系统讲解。"
   ],
   "id": "2cd264962dac6193"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 4：最优策略的有趣性质\n",
    "\n",
    "### 性质1：所有最优策略共享同一个值函数\n",
    "\n",
    "> 如果 $\\pi^*$ 和 $\\pi^{**}$ 都是最优策略，那么：\n",
    ">\n",
    "> $$\n",
    "> V^{\\pi^*}(s) = V^{\\pi^{**}}(s) = V^*(s), \\quad \\forall s\n",
    "> $$\n",
    "\n",
    "即：最优策略可能不唯一，但它们共享同一个最优值函数。\n",
    "\n",
    "---\n",
    "\n",
    "### 性质2：贪婪策略是最优策略的一种\n",
    "\n",
    "如果某策略 $\\pi$ 满足：\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a Q^\\pi(s,a)\n",
    "$$\n",
    "\n",
    "则该策略为最优策略。\n",
    "\n",
    "这个性质是“策略改进定理”的结论基础。\n",
    "\n",
    "---\n",
    "\n",
    "### 性质3：最优策略可以是确定性的\n",
    "\n",
    "对 MDP 来说，总存在一个**确定性策略**是最优的，不需要使用随机策略（即 $\\pi^*(a|s) \\in \\{0,1\\}$）。\n",
    "\n",
    "---\n",
    "\n",
    "### 性质4：最优值函数满足唯一的贝尔曼最优方程\n",
    "\n",
    "尽管策略可能不唯一，但**最优值函数是唯一的**，并且满足唯一的贝尔曼最优方程。\n",
    "这使得我们可以**通过迭代逼近值函数**，再反推出最优策略。"
   ],
   "id": "3cb8fdfa57e545ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "\n",
    "| 概念      | 数学表达                                        | 说明         |\n",
    "| ------- | ------------------------------------------- | ---------- |\n",
    "| 最优状态值函数 | $V^*(s) = \\max_\\pi V^\\pi(s)$                | 在所有策略中选最优  |\n",
    "| 最优动作值函数 | $Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)$            | 在所有策略中选最优  |\n",
    "| 贝尔曼最优公式 | $V^*(s) = \\max_a [ R + \\gamma \\sum P V^* ]$ | 定义最优值函数的递归 |\n",
    "| 贪婪策略    | $\\pi(s) = \\arg\\max_a Q^\\pi(s,a)$            | 可实现策略改进    |\n"
   ],
   "id": "310f0f8d84057143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b65b37e1658da19a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13d833c6c0011be8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
