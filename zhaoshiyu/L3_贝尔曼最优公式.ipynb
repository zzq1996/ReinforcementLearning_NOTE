{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 第3课：贝尔曼最优公式（Bellman Optimality Equation）\n",
    "\n",
    "强化学习的最终目标是：**学习一个最优策略** $\\pi^*$，使得智能体在任意状态下都能获得最大长期回报。\n",
    "要实现这一目标，我们从策略改进的思想出发，逐步构建出**最优策略**和对应的**最优值函数**，并推导出贝尔曼最优公式。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：例子 - 如何改进策略\n",
    "\n",
    "我们回顾上节中介绍的动作值函数 $Q^\\pi(s, a)$：表示在状态 $s$ 下采取动作 $a$，并之后遵循策略 $\\pi$ 所获得的期望回报。\n",
    "\n",
    "那么，如果我们发现某个动作 $a'$ 的 $Q^\\pi(s, a')$ 明显大于其他动作，我们就可以考虑在状态 $s$ 下**改变策略，选取更优动作**。\n",
    "\n",
    "### 策略改进原则：\n",
    "\n",
    "> 对所有状态 $s$，若我们总是选择动作 $a = \\arg\\max_{a} Q^\\pi(s, a)$，则新策略比旧策略至少不差，通常更优。\n",
    "\n",
    "这种“选择最大动作”的策略称为 **贪婪策略（greedy policy）**：\n",
    "\n",
    "$$\n",
    "\\pi'(s) = \\arg\\max_a Q^\\pi(s,a)\n",
    "$$\n",
    "\n",
    "这个思想是策略迭代、值迭代等方法的基础。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：最优策略和公式推导\n",
    "\n",
    "### 最优策略定义：\n",
    "\n",
    "存在至少一个策略 $\\pi^*$，使得在所有状态 $s$ 下，状态值最大：\n",
    "\n",
    "$$\n",
    "V^{\\pi^*}(s) = \\max_\\pi V^\\pi(s), \\quad \\forall s\n",
    "$$\n",
    "\n",
    "其对应的最优值函数（optimal value function）定义如下：\n",
    "\n",
    "* **最优状态值函数：**\n",
    "\n",
    "  $$\n",
    "  V^*(s) = \\max_\\pi V^\\pi(s)\n",
    "  $$\n",
    "\n",
    "* **最优动作值函数：**\n",
    "\n",
    "  $$\n",
    "  Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### 最优贝尔曼方程（Bellman Optimality Equation）\n",
    "\n",
    "我们现在不再针对具体策略 $\\pi$，而是考虑“最优可能的动作”。这就导致贝尔曼公式中的期望转化为最大值。\n",
    "\n",
    "#### 状态值函数形式：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "#### 动作值函数形式：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "Q^*(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) \\max_{a'} Q^*(s', a')\n",
    "}\n",
    "$$\n",
    "\n",
    "这些递归公式就称为**贝尔曼最优方程（Bellman Optimality Equations）**。\n",
    "\n",
    "它是**非线性的**（包含了 max），但定义了最优策略应满足的条件。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：公式求解与最优性\n",
    "\n",
    "由于贝尔曼最优方程是**非线性方程组**，直接求解很困难，特别在状态空间较大时（例如图像、文本等）。\n",
    "常见的求解方法包括：\n",
    "\n",
    "### 3.1 值迭代（Value Iteration）\n",
    "\n",
    "基于 Bellman 最优方程的**迭代逼近算法**，从任意初始 $V_0(s)$ 开始：\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V_k(s') \\right]\n",
    "$$\n",
    "\n",
    "不断更新，直到 $V_k$ 收敛为 $V^*$。之后通过：\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "得到最优策略。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 策略迭代（Strategy Iteration）\n",
    "\n",
    "交替执行两个步骤：\n",
    "\n",
    "* **策略评估**：固定策略 $\\pi$，求 $V^\\pi(s)$\n",
    "* **策略改进**：使用 $Q^\\pi(s,a)$，得到新的 $\\pi'$\n",
    "\n",
    "这两个算法将在下一课系统讲解。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：最优策略的有趣性质\n",
    "\n",
    "### 性质1：所有最优策略共享同一个值函数\n",
    "\n",
    "> 如果 $\\pi^*$ 和 $\\pi^{**}$ 都是最优策略，那么：\n",
    ">\n",
    "> $$\n",
    "> V^{\\pi^*}(s) = V^{\\pi^{**}}(s) = V^*(s), \\quad \\forall s\n",
    "> $$\n",
    "\n",
    "即：最优策略可能不唯一，但它们共享同一个最优值函数。\n",
    "\n",
    "---\n",
    "\n",
    "### 性质2：贪婪策略是最优策略的一种\n",
    "\n",
    "如果某策略 $\\pi$ 满足：\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a Q^\\pi(s,a)\n",
    "$$\n",
    "\n",
    "则该策略为最优策略。\n",
    "\n",
    "这个性质是“策略改进定理”的结论基础。\n",
    "\n",
    "---\n",
    "\n",
    "### 性质3：最优策略可以是确定性的\n",
    "\n",
    "对 MDP 来说，总存在一个**确定性策略**是最优的，不需要使用随机策略（即 $\\pi^*(a|s) \\in \\{0,1\\}$）。\n",
    "\n",
    "---\n",
    "\n",
    "### 性质4：最优值函数满足唯一的贝尔曼最优方程\n",
    "\n",
    "尽管策略可能不唯一，但**最优值函数是唯一的**，并且满足唯一的贝尔曼最优方程。\n",
    "这使得我们可以**通过迭代逼近值函数**，再反推出最优策略。\n",
    "\n",
    "---\n",
    "\n",
    "## 小结\n",
    "\n",
    "| 概念      | 数学表达                                        | 说明         |\n",
    "| ------- | ------------------------------------------- | ---------- |\n",
    "| 最优状态值函数 | $V^*(s) = \\max_\\pi V^\\pi(s)$                | 在所有策略中选最优  |\n",
    "| 最优动作值函数 | $Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)$            | 在所有策略中选最优  |\n",
    "| 贝尔曼最优公式 | $V^*(s) = \\max_a [ R + \\gamma \\sum P V^* ]$ | 定义最优值函数的递归 |\n",
    "| 贪婪策略    | $\\pi(s) = \\arg\\max_a Q^\\pi(s,a)$            | 可实现策略改进    |\n",
    "\n"
   ],
   "id": "79fcdc4fe0ea0683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "755db033755eabf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "348a527adb314393"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b65b37e1658da19a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
