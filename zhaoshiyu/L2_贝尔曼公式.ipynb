{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 第2课：贝尔曼公式（Bellman Equation）\n",
    "\n",
    "贝尔曼公式是强化学习中最核心的数学工具，它通过**递归定义值函数**，为策略评估与策略改进提供了基础。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：例子说明 Return 的重要性\n",
    "\n",
    "在强化学习中，智能体的目标不是最大化**即时奖励**，而是最大化从当前开始的**累积回报（return）**。\n",
    "\n",
    "### 示例：走迷宫\n",
    "\n",
    "假设智能体需要从起点走到终点，每一步的奖励是 -1，终点的奖励是 +10。以下是两条路径：\n",
    "\n",
    "* 路径 A：在 5 步内走到终点，Return = $-1 \\times 4 + 10 = 6$\n",
    "* 路径 B：绕远路，10 步到终点，Return = $-1 \\times 9 + 10 = 1$\n",
    "\n",
    "虽然终点奖励相同，但路径 A 明显更优，因为其 **Return 更大**。\n",
    "\n",
    "这表明：\n",
    "\n",
    "> **回报 $G_t$** 才是衡量策略优劣的真正标准，而不是单步奖励。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：State Value 的定义（状态值函数）\n",
    "\n",
    "值函数衡量一个状态“好不好”，即从该状态开始，跟随策略 $\\pi$ 所能获得的期望回报。\n",
    "\n",
    "### 定义：\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s \\right]\n",
    "= \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} \\,\\Big|\\, s_t = s \\right]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $V^\\pi(s)$：状态 $s$ 的值（在策略 $\\pi$ 下）\n",
    "* $\\gamma \\in [0,1]$：折扣因子\n",
    "* 期望是针对**策略 $\\pi$** 所引导下的所有可能轨迹\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：贝尔曼公式的详细推导（对状态值）\n",
    "\n",
    "贝尔曼公式利用了**值函数的递归结构**：当前状态的值 = 当前期望奖励 + 未来状态的值（加折扣）。\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "V^\\pi(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} \\left[ R(s,a) + \\gamma \\mathbb{E}_{s' \\sim P(\\cdot|s,a)} [V^\\pi(s')] \\right]\n",
    "}\n",
    "$$\n",
    "\n",
    "也可展开写为：\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "### 推导直觉：\n",
    "\n",
    "1. 在状态 $s$ 下，策略选择动作 $a$，概率为 $\\pi(a|s)$\n",
    "2. 环境反馈奖励 $R(s,a)$，并转移到新状态 $s'$，概率为 $P(s'|s,a)$\n",
    "3. 在新状态 $s'$ 上继续累计回报 $V^\\pi(s')$，但要乘以折扣 $\\gamma$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：向量形式与线性方程求解\n",
    "\n",
    "当状态空间 $\\mathcal{S}$ 有限时，可将值函数表示为向量 $\\mathbf{v}$，贝尔曼公式可以写成矩阵形式：\n",
    "\n",
    "### 向量化表示：\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\mathbf{r}^\\pi + \\gamma \\mathbf{P}^\\pi \\mathbf{v}\n",
    "$$\n",
    "\n",
    "* $\\mathbf{v}$：状态值向量，维度为 $|\\mathcal{S}|$\n",
    "* $\\mathbf{r}^\\pi$：每个状态下，按策略 $\\pi$ 的期望奖励\n",
    "* $\\mathbf{P}^\\pi$：状态转移矩阵，维度为 $|\\mathcal{S}| \\times |\\mathcal{S}|$\n",
    "\n",
    "### 解法（线性代数）：\n",
    "\n",
    "将上式移项：\n",
    "\n",
    "$$\n",
    "(I - \\gamma \\mathbf{P}^\\pi)\\mathbf{v} = \\mathbf{r}^\\pi\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\boxed{\n",
    "\\mathbf{v} = (I - \\gamma \\mathbf{P}^\\pi)^{-1} \\mathbf{r}^\\pi\n",
    "}\n",
    "$$\n",
    "\n",
    "适用于小规模状态空间，但在大规模（例如图像状态）中不可行，因此需要后续课程介绍的**迭代方法**（如值迭代、策略迭代等）。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5：Action Value 的定义（动作值函数）\n",
    "\n",
    "状态值函数 $V^\\pi(s)$ 描述“在状态 $s$ 开始的好坏”；而**动作值函数** $Q^\\pi(s,a)$ 描述“在状态 $s$，执行动作 $a$ 后的好坏”。\n",
    "\n",
    "### 定义：\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s, a_t = a \\right]\n",
    "= \\mathbb{E} \\left[ r_{t+1} + \\gamma V^\\pi(s_{t+1}) \\,\\big|\\, s_t = s, a_t = a \\right]\n",
    "$$\n",
    "\n",
    "或递归式写为：\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s')\n",
    "$$\n",
    "\n",
    "### 状态值与动作值的关系：\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)\n",
    "$$\n",
    "\n",
    "这意味着：状态值是**在当前策略下，对动作值函数的加权平均**。\n",
    "\n",
    "---\n",
    "\n",
    "### 小结\n",
    "\n",
    "| 函数                 | 解释                       | 数学定义                                                 |                       |\n",
    "| ------------------ | ------------------------ | ---------------------------------------------------- | --------------------- |\n",
    "| 状态值函数 $V^\\pi(s)$   | 在状态 $s$ 下，期望的长期奖励        | ( \\mathbb{E}\\_\\pi\\[G\\_t                              | s\\_t = s] )           |\n",
    "| 动作值函数 $Q^\\pi(s,a)$ | 在状态 $s$ 下采取动作 $a$ 后的长期奖励 | ( \\mathbb{E}\\_\\pi\\[G\\_t                              | s\\_t = s, a\\_t = a] ) |\n",
    "| 贝尔曼方程              | 定义值函数的递归关系               | $V^\\pi(s) = \\mathbb{E}_{a,s'}[R + \\gamma V^\\pi(s')]$ |                       |\n",
    "\n"
   ],
   "id": "e0a17710dde04935"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8c053c535f846bef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5030b8460d42b3b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
