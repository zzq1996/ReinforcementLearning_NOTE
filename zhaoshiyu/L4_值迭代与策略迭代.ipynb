{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# 第4课：值迭代与策略迭代（Value Iteration & Policy Iteration）\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：值迭代算法（Value Iteration）\n",
    "\n",
    "值迭代是基于贝尔曼最优方程（Bellman Optimality Equation）的迭代方法，其核心思想是：\n",
    "\n",
    "> 利用 Bellman 最优方程反复更新状态值函数 $V(s)$，直到收敛为 $V^*(s)$，然后从中提取最优策略。\n",
    "\n",
    "![](./img/4_1.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 算法形式（同步更新）：\n",
    "\n",
    "初始化 $V_0(s)$ 为任意值（例如 0），然后迭代如下更新：\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) = \\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V_k(s') \\right]\n",
    "$$\n",
    "\n",
    "直到 $|V_{k+1}(s) - V_k(s)| < \\epsilon$，即值函数收敛。\n",
    "\n",
    "---\n",
    "\n",
    "### 最终策略提取：\n",
    "\n",
    "值函数收敛后，通过贪婪原则获得策略：\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 伪代码：\n",
    "\n",
    "```python\n",
    "# Value Iteration\n",
    "Initialize V(s) arbitrarily\n",
    "Repeat:\n",
    "    Δ ← 0\n",
    "    For each state s:\n",
    "        v ← V(s)\n",
    "        V(s) ← max_a [ R(s,a) + γ Σ_s' P(s'|s,a) V(s') ]\n",
    "        Δ ← max(Δ, |v - V(s)|)\n",
    "Until Δ < θ (convergence threshold)\n",
    "Return optimal policy π(s) = argmax_a [ R(s,a) + γ Σ_s' P(s'|s,a) V(s') ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 特点：\n",
    "\n",
    "| 优点             | 缺点                        |\n",
    "| -------------- | ------------------------- |\n",
    "| 易于实现，快速逼近最优值函数 | 每次都需要执行完整的“最大化”操作（策略隐式更新） |\n",
    "| 不需要明确策略更新步骤    | 对大规模状态空间，计算量大             |\n",
    "\n",
    "\n"
   ],
   "id": "cab25302aec9d5fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2：策略迭代算法（Policy Iteration）\n",
    "\n",
    "策略迭代是一种**显式维护和更新策略**的方法，交替进行两个阶段：\n",
    "\n",
    "1. **策略评估（Policy Evaluation）**：在固定策略 $\\pi$ 下求解 $V^\\pi(s)$\n",
    "2. **策略改进（Policy Improvement）**：对所有状态执行贪婪更新，得到新策略 $\\pi'(s)$\n",
    "\n",
    "重复这两个步骤直到策略收敛。\n",
    "\n",
    "![](./img/4_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 步骤详解：\n",
    "\n",
    "#### ① 策略评估(policy evaluation)：\n",
    "\n",
    "给定策略 $\\pi$，求解其值函数：\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "可以：\n",
    "\n",
    "* 精确求解线性方程组\n",
    "* 或使用迭代逼近（如 TD(0) 或近似方法）\n",
    "\n",
    "#### ② 策略改进(policy improvement)：\n",
    "\n",
    "$$\n",
    "\\pi_{\\text{new}}(s) = \\arg\\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 伪代码：\n",
    "\n",
    "```python\n",
    "# Policy Iteration\n",
    "Initialize policy π(s) arbitrarily\n",
    "Repeat:\n",
    "    Evaluate V^π by solving Bellman equation for π\n",
    "    For each state s:\n",
    "        π_new(s) ← argmax_a [ R(s,a) + γ Σ_s' P(s'|s,a) V^π(s') ]\n",
    "    If π_new == π:\n",
    "        break\n",
    "    Else:\n",
    "        π ← π_new\n",
    "Return π\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 特点：\n",
    "\n",
    "| 优点         | 缺点                 |\n",
    "| ---------- | ------------------ |\n",
    "| 通常收敛快，策略稳定 | 每轮评估开销大（需要精确或近似求解） |\n",
    "| 每轮策略明确改进   | 策略评估若不收敛则可能不稳定     |\n",
    "\n"
   ],
   "id": "4f1a009b38002d02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 3：截断策略迭代算法（Truncated Policy Iteration）\n",
    "\n",
    "为了兼顾值迭代的**快速更新**和策略迭代的**策略改进效率**，提出了**截断策略迭代**（也叫 modified policy iteration）。\n",
    "\n",
    "![](./img/4_3.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 基本思想：\n",
    "\n",
    "> 在策略迭代中，策略评估不需要完全收敛，只执行 **有限轮迭代更新**（如 n 步）后就进行策略改进。\n",
    "\n",
    "这样就能：\n",
    "\n",
    "* 节省每轮的计算时间\n",
    "* 更快进入策略更新阶段\n",
    "\n",
    "---\n",
    "\n",
    "### 算法结构：\n",
    "\n",
    "```python\n",
    "# Truncated Policy Iteration\n",
    "Initialize π(s), V(s)\n",
    "Repeat:\n",
    "    # Policy Evaluation with n iterations\n",
    "    For i in range(n):\n",
    "        For each state s:\n",
    "            V(s) ← Σ_a π(a|s) [ R(s,a) + γ Σ_s' P(s'|s,a) V(s') ]\n",
    "\n",
    "    # Policy Improvement\n",
    "    For each state s:\n",
    "        π(s) ← argmax_a [ R(s,a) + γ Σ_s' P(s'|s,a) V(s') ]\n",
    "Until π converges\n",
    "Return π\n",
    "```\n",
    "\n",
    "n 通常为 1\\~5，根据任务大小调节。\n",
    "\n",
    "---\n",
    "\n",
    "### 特点：\n",
    "\n",
    "| 优点               | 缺点             |\n",
    "| ---------------- | -------------- |\n",
    "| 较值迭代更稳定，较策略迭代更高效 | 参数 $n$ 需要调节    |\n",
    "| 实际中常用于平衡开销和收敛速度  | 仍需存储完整的策略和价值函数 |\n",
    "\n"
   ],
   "id": "a5d8a0da966c2d9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "\n",
    "| 算法     | 更新目标    | 每步计算         | 收敛速度   | 是否显式维护策略 |\n",
    "| ------ | ------- | ------------ | ------ | -------- |\n",
    "| 值迭代    | $V^*$   | 对所有状态执行最大化   | 快（通常）  | 否（策略后提取） |\n",
    "| 策略迭代   | $\\pi^*$ | 策略评估 + 贪婪更新  | 稳定但可能慢 | 是        |\n",
    "| 截断策略迭代 | $\\pi^*$ | 策略评估不完全 + 更新 | 平衡     | 是        |"
   ],
   "id": "ec7f3b88649afd05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd226006085c1bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d2f18e835cede8c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
