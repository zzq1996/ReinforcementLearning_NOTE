{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# 第10课：Actor-Critic方法\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：最简单的 Actor-Critic（QAC）\n",
    "\n",
    "### 基本思想：\n",
    "\n",
    "* **Actor**：负责策略 $\\pi_\\theta(a|s)$ 的参数化与更新\n",
    "* **Critic**：负责估计动作值函数 $Q_w(s,a)$（或状态值函数 $V_w(s)$），为Actor提供策略梯度估计的基准\n",
    "\n",
    "---\n",
    "\n",
    "### QAC算法框架：\n",
    "\n",
    "* Critic使用TD方法估计 $Q_w(s,a)$\n",
    "* Actor基于Critic的估计，用梯度上升优化策略参数\n",
    "\n",
    "---\n",
    "\n",
    "### 关键更新：\n",
    "\n",
    "* **Critic** 更新：\n",
    "\n",
    "$$\n",
    "w \\leftarrow w + \\beta \\delta_t \\nabla_w Q_w(s_t, a_t)\n",
    "$$\n",
    "\n",
    "其中，TD误差\n",
    "\n",
    "$$\n",
    "\\delta_t = r_{t+1} + \\gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)\n",
    "$$\n",
    "\n",
    "* **Actor** 更新：\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 解释：\n",
    "\n",
    "* Critic评估策略的优劣，提供TD误差作为信号\n",
    "* Actor利用TD误差调整策略，使策略向更好方向优化\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：Advantage Actor-Critic (A2C)\n",
    "\n",
    "### 优化点：\n",
    "\n",
    "用 **优势函数** 替代动作值函数，减少策略梯度的方差，提高学习稳定性。\n",
    "\n",
    "---\n",
    "\n",
    "### 计算优势函数：\n",
    "\n",
    "$$\n",
    "A(s,a) = Q(s,a) - V(s)\n",
    "$$\n",
    "\n",
    "* $Q(s,a)$：动作值估计\n",
    "* $V(s)$：状态值估计\n",
    "\n",
    "---\n",
    "\n",
    "### A2C核心更新：\n",
    "\n",
    "* Critic估计状态值 $V_w(s)$\n",
    "* Actor更新利用优势函数：\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha A(s_t, a_t) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
    "$$\n",
    "\n",
    "* Critic的目标：\n",
    "\n",
    "$$\n",
    "\\min_w \\left( r_{t+1} + \\gamma V_w(s_{t+1}) - V_w(s_t) \\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 特点：\n",
    "\n",
    "* 减少了方差，提升样本效率\n",
    "* 广泛应用于深度强化学习框架\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：重要性采样和 Off-policy Actor-Critic\n",
    "\n",
    "### 问题：\n",
    "\n",
    "* Actor-Critic多为**on-policy**方法\n",
    "* 离线或异策略数据无法直接利用\n",
    "\n",
    "---\n",
    "\n",
    "### 解决方案：重要性采样（Importance Sampling）\n",
    "\n",
    "调整样本权重：\n",
    "\n",
    "$$\n",
    "w_t = \\frac{\\pi_\\theta(a_t|s_t)}{\\mu(a_t|s_t)}\n",
    "$$\n",
    "\n",
    "* $\\mu$：行为策略\n",
    "* $\\pi_\\theta$：目标策略\n",
    "\n",
    "---\n",
    "\n",
    "### Off-policy Actor-Critic方法：\n",
    "\n",
    "* 利用重要性采样权重校正策略梯度\n",
    "* 结合Replay Buffer，实现数据复用和稳定训练\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：Deterministic Actor-Critic (DPG)\n",
    "\n",
    "### 背景：\n",
    "\n",
    "* 传统策略梯度为随机策略\n",
    "* 对连续动作空间，确定性策略能减少方差，提高效率\n",
    "\n",
    "---\n",
    "\n",
    "### 确定性策略：\n",
    "\n",
    "$$\n",
    "\\mu_\\theta(s) = a\n",
    "$$\n",
    "\n",
    "策略直接输出动作，不是概率分布。\n",
    "\n",
    "---\n",
    "\n",
    "### DPG算法核心：\n",
    "\n",
    "* Critic估计 $Q(s,a)$\n",
    "* Actor更新利用Critic的梯度：\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu} \\left[ \\nabla_\\theta \\mu_\\theta(s) \\nabla_a Q^\\mu(s,a) \\big|_{a=\\mu_\\theta(s)} \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 特点：\n",
    "\n",
    "* 适合高维连续动作空间\n",
    "* 结合经验回放和目标网络实现稳定训练（即 DDPG 算法）\n",
    "\n",
    "---\n",
    "\n",
    "## 本课总结\n",
    "\n",
    "* Actor-Critic方法有效结合策略优化与价值估计\n",
    "* A2C用优势函数降低方差，提升性能\n",
    "* 重要性采样支持离策略训练\n",
    "* DPG针对连续动作提出确定性策略，提升效率\n",
    "\n",
    "\n"
   ],
   "id": "3d13fcfa34da09b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b0fbf10fd4e9cb6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a06b3bfdcb9c96a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c0211cceaaa327ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
