{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 第1课：基本概念\n",
    "\n",
    "强化学习（Reinforcement Learning, RL）是让智能体（agent）在一个环境（environment）中与其互动、通过获得奖励（reward）来学习最优决策策略的一种学习方法。\n",
    "\n",
    "本节主要介绍强化学习中的核心要素和建模方式，包括状态、动作、策略、奖励、回报，以及马尔可夫决策过程（MDP）等基本概念。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：State, Action, Policy 等\n",
    "\n",
    "### 1.1 状态（State）与状态空间（State Space）\n",
    "\n",
    "* 状态 $s \\in \\mathcal{S}$ 表示环境在某一时刻的描述，是智能体做决策所依据的信息。\n",
    "* 状态空间 $\\mathcal{S}$ 可以是有限的离散集合（如棋盘位置）或连续空间（如小车坐标与速度）。\n",
    "\n",
    "**例子：**\n",
    "\n",
    "* 下围棋时，状态是当前棋盘的布局。\n",
    "* 在自动驾驶中，状态可能是车辆的速度、加速度、前方距离等组合特征。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 动作（Action）与动作空间（Action Space）\n",
    "\n",
    "* 动作 $a \\in \\mathcal{A}(s)$ 是智能体在状态 $s$ 下可采取的行为选择。\n",
    "* 动作空间 $\\mathcal{A}$ 可以是离散集合（如上下左右移动），也可以是连续的（如转向角度为任意实数）。\n",
    "\n",
    "**例子：**\n",
    "\n",
    "* 在 Atari 游戏中，动作是按哪个键。\n",
    "* 在控制任务中，动作可能是一个实数力的施加方向。\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 策略（Policy）\n",
    "\n",
    "策略是智能体在每个状态下选择动作的行为准则，是强化学习的核心目标之一。\n",
    "\n",
    "有两种策略表示方式：\n",
    "\n",
    "* **确定性策略（Deterministic Policy）**\n",
    "\n",
    "  $$\n",
    "  a = \\pi(s)\n",
    "  $$\n",
    "\n",
    "  表示在状态 $s$ 下总是选择动作 $a$。\n",
    "\n",
    "* **随机性策略（Stochastic Policy）**\n",
    "\n",
    "  $$\n",
    "  \\pi(a|s) = P(a|s)\n",
    "  $$\n",
    "\n",
    "  表示在状态 $s$ 下以概率 $P(a|s)$ 选择动作 $a$。\n",
    "\n",
    "策略的目标：最大化长期回报 $G_t$。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9a0d7af0c62e8d45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2：Reward, Return, MDP 等\n",
    "\n",
    "### 2.1 奖励（Reward）\n",
    "\n",
    "* 奖励 $r_t$ 是环境在时间步 $t$ 给智能体的反馈信号，用来评价当前动作的好坏。\n",
    "* 奖励值可以是正（鼓励）、负（惩罚）或零（中性）。\n",
    "\n",
    "**注意：**\n",
    "\n",
    "* 奖励是局部即时信号，不等于长期好坏。\n",
    "* 智能体的学习目标通常是**累计奖励最大化**，而非单步奖励最大化。\n",
    "\n",
    "---\n",
    "\n",
    "你写得已经很清晰了，我在此基础上为你**补充“折扣回报（discounted return）”的深入解释与注解**，让这部分更加完整：\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 回报（Return）\n",
    "\n",
    "> 回报 $G_t$ 是从时间步 $t$ 开始，智能体之后所获得的所有奖励的**加权和**。\n",
    "\n",
    "强化学习中主要有两种任务类型，对应两种回报的定义方式：\n",
    "\n",
    "---\n",
    "\n",
    "#### **有限期任务（episodic task）**：\n",
    "\n",
    "* 任务在有限步后终止（如走出迷宫、下完一盘棋）\n",
    "\n",
    "$$\n",
    "G_t = r_{t+1} + r_{t+2} + \\cdots + r_T\n",
    "$$\n",
    "\n",
    "其中 $T$ 是 episode 的终止时间步。\n",
    "\n",
    "---\n",
    "\n",
    "#### **无限期任务（continuing task）**：\n",
    "\n",
    "* 智能体的行为永不终止（如机器人巡逻、投资策略）\n",
    "* 为避免回报无限大，引入 **折扣因子 $\\gamma \\in [0, 1)$** 控制未来奖励的重要性\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2.1 折扣回报（Discounted Return）\n",
    "\n",
    "折扣回报是无限期任务中**最常用的回报定义**，也是后续值函数、Bellman方程等概念的基础。\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n",
    "}\n",
    "$$\n",
    "\n",
    "##### 为什么引入折扣因子？\n",
    "\n",
    "1. **收敛性**：若奖励始终为正常数，则不加折扣会导致 $G_t \\to \\infty$\n",
    "2. **不确定性建模**：未来不可知，故不应与当前奖励等价对待\n",
    "3. **偏好近期奖励**：尤其在工程中更关心短期反馈（例如机器人避障）\n",
    "\n",
    "---\n",
    "\n",
    "##### 折扣因子的取值含义：\n",
    "\n",
    "| $\\gamma$ 值         | 解释             |\n",
    "| ------------------ | -------------- |\n",
    "| $\\gamma = 0$       | 仅考虑眼前奖励（即贪婪）   |\n",
    "| $\\gamma = 0.5$     | 中等程度考虑未来       |\n",
    "| $\\gamma \\approx 1$ | 长期回报导向（如围棋、金融） |\n",
    "\n",
    "通常设为 0.9\\~0.99。\n",
    "\n",
    "---\n",
    "\n",
    "### 小结：\n",
    "\n",
    "* **回报 $G_t$** 是智能体学习的基础信号\n",
    "* **折扣回报**使得学习过程**可收敛、可控、可解释**\n",
    "* 后续的 **状态值函数** $V^\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]$ 和 **贝尔曼方程** 都基于折扣回报\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 马尔可夫决策过程（Markov Decision Process, MDP）\n",
    "\n",
    "强化学习的问题通常建模为一个 MDP，即一个五元组：\n",
    "\n",
    "$$\n",
    "(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)\n",
    "$$\n",
    "\n",
    "* $\\mathcal{S}$：状态空间\n",
    "* $\\mathcal{A}$：动作空间\n",
    "* $P(s'|s,a)$：状态转移概率，表示在状态 $s$ 采取动作 $a$ 后转移到下一个状态 $s'$ 的概率\n",
    "* $R(s,a)$：奖励函数，表示在状态 $s$ 下采取动作 $a$ 所得到的期望奖励\n",
    "* $\\gamma$：折扣因子，控制未来奖励的影响力\n",
    "\n",
    "**马尔可夫性质**：\n",
    "未来状态 $s_{t+1}$ 只依赖于当前状态 $s_t$ 和当前动作 $a_t$，与过去状态和动作无关：\n",
    "\n",
    "$$\n",
    "P(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},...) = P(s_{t+1}|s_t,a_t)\n",
    "$$\n",
    "\n"
   ],
   "id": "86fdff6bc8f6c9b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 总结\n",
    "\n",
    "| 概念  | 符号                                         | 含义                   |\n",
    "| --- | ------------------------------------------ | -------------------- |\n",
    "| 状态  | $s$                                        | 当前环境的描述              |\n",
    "| 动作  | $a$                                        | 智能体对状态的响应行为          |\n",
    "| 策略  | $\\pi$                                      | 映射状态到动作的函数（可能是概率分布）  |\n",
    "| 奖励  | $r$                                        | 环境给予智能体的反馈           |\n",
    "| 回报  | $G_t$                                      | 智能体从时间步 $t$ 起获得的累积奖励 |\n",
    "| MDP | ($\\mathcal{S}, \\mathcal{A}, P, R, \\gamma$) | 强化学习的建模框架            |\n"
   ],
   "id": "e677a10117f7fa22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Q1 episodes 与 trajectories 的关系，以及它们在强化学习中的角色和区别。\n",
    "\n",
    "---\n",
    "\n",
    "## 一句话总结\n",
    "\n",
    "> **Trajectory（轨迹）** 是一条完整的状态-动作-奖励序列，**Episode（回合）** 是一种特殊类型的轨迹，指**从初始状态到终止状态的轨迹**。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Trajectory（轨迹）\n",
    "\n",
    "### 定义：\n",
    "\n",
    "* 轨迹是智能体与环境交互过程中产生的**一个完整序列**，通常包含状态、动作、奖励等：\n",
    "\n",
    "  $$\n",
    "  \\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \\ldots, s_T)\n",
    "  $$\n",
    "* 轨迹可以是：\n",
    "\n",
    "  * **有限的**：以终止状态 $s_T$ 结束（见 episode）\n",
    "  * **无限的**：例如在持续性任务中（continuing task），轨迹可以一直延续下去，不一定终止\n",
    "\n",
    "### 表达形式：\n",
    "\n",
    "* 完整轨迹（含奖励）：\n",
    "\n",
    "  $$\n",
    "  \\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \\dots, s_T)\n",
    "  $$\n",
    "* 简写形式（不含奖励）：\n",
    "\n",
    "  $$\n",
    "  \\tau = (s_0, a_0, s_1, a_1, \\dots, s_T)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Episode（回合）\n",
    "\n",
    "### 定义：\n",
    "\n",
    "* Episode 是从环境初始状态开始，到\\*\\*达到终止状态（terminal state）\\*\\*为止的一条轨迹。\n",
    "* 它是**有限轨迹的一种**，适用于“任务有明确起止”的场景（如：下棋、闯关游戏等）。\n",
    "\n",
    "### 举例：\n",
    "\n",
    "* 在走迷宫任务中，一次成功或失败走出迷宫都可视为一个 episode。\n",
    "* 在 Atari 游戏中，游戏结束或生命用完即 episode 结束。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 它们的关系与区别\n",
    "\n",
    "| 比较项    | Trajectory（轨迹） | Episode（回合）        |\n",
    "| ------ | -------------- | ------------------ |\n",
    "| 是否终止   | 可以终止也可以不终止     | 必须在终止状态结束          |\n",
    "| 长度     | 可有限或无限         | 有限长度               |\n",
    "| 应用场景   | 所有任务都可使用       | 通常用于 episodic task |\n",
    "| 是否包含奖励 | 可以包含也可以不包含     | 通常包含奖励             |\n",
    "\n",
    " **结论**：\n",
    "\n",
    "* 所有 episode 都是 trajectory，但不是所有 trajectory 都是 episode。\n",
    "* 在实现策略优化（如 REINFORCE）或训练经验回放（如 DQN）时，都需要采样 trajectory（有时用 episodic trajectory）。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 补充术语\n",
    "\n",
    "* **Partial Trajectory / Fragment**：轨迹的一部分，如 $(s_t, a_t, r_{t+1}, s_{t+1})$\n",
    "* **Trajectory Segment**：若干连续步骤构成的轨迹片段，常用于 n-step 方法。\n",
    "* **Trajectory Distribution**：在策略 $\\pi$ 下，所有可能轨迹的分布（在策略梯度中非常重要）：\n",
    "\n",
    "  $$\n",
    "  p_\\pi(\\tau) = p(s_0)\\prod_{t=0}^{T-1} \\pi(a_t|s_t) P(s_{t+1}|s_t,a_t)\n",
    "  $$\n",
    "\n",
    "\n"
   ],
   "id": "7a06a6a6aff61b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ddd4ab15e63347e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
