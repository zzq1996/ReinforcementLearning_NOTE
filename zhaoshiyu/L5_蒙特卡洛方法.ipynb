{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# 第5课：蒙特卡洛方法（Monte Carlo Methods）\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：通过例子介绍蒙特卡洛\n",
    "\n",
    "在前几课中，我们通过 MDP 的状态转移概率 $P(s'|s,a)$ 来精确地计算值函数。但现实世界中，环境模型往往是**未知的**，这时我们可以：\n",
    "\n",
    "> 通过**与环境交互得到的实际轨迹**，直接估计值函数 —— 这就是蒙特卡洛方法。\n",
    "\n",
    "---\n",
    "\n",
    "### 例子：玩 21 点（Blackjack）\n",
    "\n",
    "我们不知道 21 点的全部概率模型，但可以让智能体玩很多局游戏，然后：\n",
    "\n",
    "* 对每种状态 $s$，记录其在轨迹中的回报 $G_t$\n",
    "* 估计状态值 $V(s)$ 为所有回报的平均值\n",
    "\n",
    "例如，状态 `s = 18分 + 无A` 出现了 5 次，分别得到回报 0, 1, -1, 1, 1，那么估计值：\n",
    "\n",
    "$$\n",
    "V(s) = \\frac{0 + 1 - 1 + 1 + 1}{5} = 0.4\n",
    "$$\n",
    "\n",
    "这就是\\*\\*蒙特卡洛评估（Monte Carlo Evaluation）\\*\\*的基本思想。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：MC Basic 算法介绍（评估）\n",
    "\n",
    "### 目标：\n",
    "\n",
    "> 给定一个策略 $\\pi$，估计它的状态值函数 $V^\\pi(s)$，或动作值函数 $Q^\\pi(s,a)$\n",
    "\n",
    "---\n",
    "\n",
    "### 核心思想：\n",
    "\n",
    "通过执行多次 episode（完整轨迹），用**实际观测到的 return** 来计算值函数。\n",
    "\n",
    "---\n",
    "\n",
    "### 主要方法：\n",
    "\n",
    "* **First-visit MC**：每次 episode 中，只对某状态第一次出现的时间步更新\n",
    "* **Every-visit MC**：每次 episode 中，对该状态所有出现的时间步都更新\n",
    "\n",
    "---\n",
    "\n",
    "### First-Visit MC 算法伪代码（估计 $V^\\pi(s)$）：\n",
    "\n",
    "```python\n",
    "Initialize: V(s) = 0, Returns(s) = []\n",
    "For episode = 1 to N:\n",
    "    Generate an episode: (s0, a0, r1, s1, a1, r2, ..., sT)\n",
    "    G = 0\n",
    "    For t = T-1 to 0:\n",
    "        G ← γ * G + r_{t+1}\n",
    "        If s_t is the first occurrence in episode:\n",
    "            Append G to Returns(s_t)\n",
    "            V(s_t) = average(Returns(s_t))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：MC Basic 算法例子\n",
    "\n",
    "我们用一个简单例子说明 first-visit MC 如何估计 $V(s)$：\n",
    "\n",
    "---\n",
    "\n",
    "### 假设：\n",
    "\n",
    "* 状态空间：$s \\in \\{A, B\\}$\n",
    "* 策略 $\\pi$ 固定，允许我们采样 episode\n",
    "\n",
    "---\n",
    "\n",
    "### Episode 1:\n",
    "\n",
    "$$\n",
    "(s_0 = A, a_0, r_1 = 1, s_1 = B, a_1, r_2 = 0, s_2 = Term)\n",
    "$$\n",
    "\n",
    "→ $G_0 = 1$, $G_1 = 0$\n",
    "\n",
    "* First-visit：\n",
    "\n",
    "  * $A$ 首次出现在 $t=0$，加入 $G=1$\n",
    "  * $B$ 首次出现在 $t=1$，加入 $G=0$\n",
    "\n",
    "---\n",
    "\n",
    "### Episode 2:\n",
    "\n",
    "$$\n",
    "(s_0 = A, a_0, r_1 = 0, s_1 = A, a_1, r_2 = 2, s_2 = Term)\n",
    "$$\n",
    "\n",
    "→ $G_0 = 0 + 2 = 2$, $G_1 = 2$\n",
    "\n",
    "* First-visit：\n",
    "\n",
    "  * $A$ 首次出现在 $t=0$，加入 $G=2$\n",
    "\n",
    "---\n",
    "\n",
    "### 更新值：\n",
    "\n",
    "* $V(A) = \\frac{1 + 2}{2} = 1.5$\n",
    "* $V(B) = \\frac{0}{1} = 0$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：MC Exploring Starts 算法（控制）\n",
    "\n",
    "### 背景：\n",
    "\n",
    "前面的 MC 只能评估**已知策略**，但我们还希望**优化策略** —— 即学出最优策略。\n",
    "\n",
    "这就需要探索所有状态-动作对，而不是只采样固定策略。\n",
    "\n",
    "---\n",
    "\n",
    "### Exploring Starts 假设：\n",
    "\n",
    "> 每个 episode 从**任意状态-动作对 $(s, a)$** 开始，并有非零概率访问所有可能的 $s, a$。\n",
    "\n",
    "在这个假设下，我们可以收集所有 $Q(s,a)$ 的估计值，并构造最优策略。\n",
    "\n",
    "---\n",
    "\n",
    "### 算法结构：\n",
    "\n",
    "```python\n",
    "Initialize: Q(s,a) arbitrarily, π(s) = argmax_a Q(s,a)\n",
    "Loop:\n",
    "    Choose (s0, a0) with Exploring Starts\n",
    "    Generate an episode following π starting with (s0, a0)\n",
    "    For each (s, a) in episode:\n",
    "        G = return following first occurrence of (s,a)\n",
    "        Update Q(s,a) as average of returns\n",
    "    Update π(s) = argmax_a Q(s,a)\n",
    "```\n",
    "\n",
    "* 策略始终对当前的 $Q(s,a)$ 贪婪\n",
    "* 不断迭代更新 $Q$ 与 $\\pi$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5：MC ε-Greedy 算法介绍（控制）\n",
    "\n",
    "在真实任务中，“任意状态动作起始”往往难以满足，因此引入**ε-Greedy 策略探索机制**：\n",
    "\n",
    "---\n",
    "\n",
    "### ε-Greedy 策略：\n",
    "\n",
    "$$\n",
    "\\pi(a|s) =\n",
    "\\begin{cases}\n",
    "1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}(s)|}, & \\text{if } a = \\arg\\max Q(s,a) \\\\\n",
    "\\frac{\\varepsilon}{|\\mathcal{A}(s)|}, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* 以概率 $1 - \\varepsilon$ 选择当前最优动作（贪婪）\n",
    "* 以概率 $\\varepsilon$ 随机选择其他动作（探索）\n",
    "\n",
    "---\n",
    "\n",
    "### 算法结构（on-policy learning）：\n",
    "\n",
    "```python\n",
    "Initialize: Q(s,a), π(s) ε-greedy w.r.t. Q\n",
    "Loop:\n",
    "    Generate episode using π (with ε-greedy exploration)\n",
    "    For each (s,a) in episode:\n",
    "        G = return following first occurrence of (s,a)\n",
    "        Update Q(s,a) as average of returns\n",
    "    Update π(s) = ε-greedy w.r.t. Q(s,a)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 特点：\n",
    "\n",
    "| 特性           | 说明                     |\n",
    "| ------------ | ---------------------- |\n",
    "| on-policy    | 策略使用自身数据改进自己           |\n",
    "| 保证所有动作都有非零概率 | 避免陷入局部最优               |\n",
    "| 易于实现         | 比 Exploring Starts 更实用 |\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6：MC ε-Greedy 算法例子\n",
    "\n",
    "假设一个小型迷宫任务：\n",
    "\n",
    "* 状态 $s \\in \\{A, B, C\\}$，动作 $a \\in \\{L, R\\}$\n",
    "* 初始 $Q(s,a) = 0$，ε = 0.1\n",
    "\n",
    "经过多次 episode 后：\n",
    "\n",
    "* 记录状态-动作对的所有 return 平均值更新 Q\n",
    "* ε-greedy 策略选择 Q 最大的动作为主，偶尔探索\n",
    "\n",
    "最终：\n",
    "\n",
    "* $Q(s,a)$ 趋于真实值函数 $Q^*(s,a)$\n",
    "* 策略 $\\pi$ 趋于最优策略 $\\pi^*$\n",
    "\n",
    "---\n",
    "\n",
    "## 小结\n",
    "\n",
    "| 算法                       | 是否需要模型 | 是否用于控制  | 探索机制               |\n",
    "| ------------------------ | ------ | ------- | ------------------ |\n",
    "| First-Visit MC           | 否      | 否（策略评估） | 无                  |\n",
    "| MC with Exploring Starts | 否      | 是（策略改进） | 每次随机起始             |\n",
    "| MC with ε-Greedy         | 否      | 是（策略改进） | ε-Greedy on-policy |\n",
    "\n",
    "\n"
   ],
   "id": "c88ca49360702ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12418ce654789f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a594098b7aded36b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69cfef9acbe4978a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9a9ac0782afca3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
