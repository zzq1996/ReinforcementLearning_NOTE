{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 第9课：策略梯度方法（Policy Gradient Methods）\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：该方法的基本思路\n",
    "\n",
    "传统强化学习多通过**值函数间接优化策略**（如Q-learning、SARSA），而**策略梯度方法**则：\n",
    "\n",
    "> **直接对策略参数 $\\theta$ 优化，最大化长期期望回报**\n",
    "\n",
    "---\n",
    "\n",
    "### 为什么用策略梯度？\n",
    "\n",
    "* 能处理**连续动作空间**\n",
    "* 适合**随机策略**，有利于探索\n",
    "* 理论上能避免值函数估计误差引入的偏差\n",
    "* 易于与函数逼近（神经网络）结合\n",
    "\n",
    "---\n",
    "\n",
    "### 策略参数化：\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(a|s) = \\text{策略函数，依赖参数 } \\theta\n",
    "$$\n",
    "\n",
    "目标是求：\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\max_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：该方法的目标函数 1 — Average Value（起点状态）\n",
    "\n",
    "---\n",
    "\n",
    "定义：\n",
    "\n",
    "$$\n",
    "J(\\theta) = V^{\\pi_\\theta}(s_0) = \\mathbb{E}_{\\pi_\\theta}[ G_0 | s_0 ]\n",
    "$$\n",
    "\n",
    "即从固定起点 $s_0$ 出发，策略 $\\pi_\\theta$ 下的期望累计回报。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：该方法的目标函数 2 — Average Reward（稳态分布）\n",
    "\n",
    "---\n",
    "\n",
    "当任务为无终止马尔可夫过程，定义稳态分布 $d^{\\pi_\\theta}(s)$：\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_s d^{\\pi_\\theta}(s) \\sum_a \\pi_\\theta(a|s) r(s,a)\n",
    "$$\n",
    "\n",
    "即长期平均奖励。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：目标函数的梯度计算\n",
    "\n",
    "关键在于推导 $\\nabla_\\theta J(\\theta)$，经典\\*\\*策略梯度定理（Policy Gradient Theorem）\\*\\*给出：\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ Q^{\\pi_\\theta}(s,a) \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 解释：\n",
    "\n",
    "* 利用对数梯度技巧 $\\nabla_\\theta \\pi_\\theta(a|s) = \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s)$\n",
    "* 期望是对策略下的状态-动作对采样\n",
    "\n",
    "---\n",
    "\n",
    "### 常用变体：\n",
    "\n",
    "* 用优势函数替代 $Q^{\\pi_\\theta}(s,a)$ 来减小方差\n",
    "* 使用蒙特卡洛采样或时序差分估计 $Q$ 值\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5：梯度上升算法和REINFORCE\n",
    "\n",
    "### REINFORCE 算法（Williams, 1992）\n",
    "\n",
    "基于蒙特卡洛采样，利用完整轨迹回报估计 $Q$：\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k\n",
    "$$\n",
    "\n",
    "更新参数：\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 算法步骤：\n",
    "\n",
    "1. 采样完整 episode $(s_0,a_0,r_1,s_1,a_1,r_2,\\dots)$\n",
    "2. 对每个时间步 $t$ 计算回报 $G_t$\n",
    "3. 计算梯度 $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n",
    "4. 累积梯度更新 $\\theta$\n",
    "\n",
    "---\n",
    "\n",
    "### 优点：\n",
    "\n",
    "* 简单，易于实现\n",
    "* 理论保证收敛\n",
    "\n",
    "### 缺点：\n",
    "\n",
    "* 方差较大，收敛较慢\n",
    "* 需要完整 episode，不能在线更新\n",
    "\n",
    "---\n",
    "\n",
    "## 本课总结\n",
    "\n",
    "* 策略梯度方法通过参数化策略直接优化目标函数\n",
    "* 策略梯度定理提供了梯度表达式，方便计算\n",
    "* REINFORCE 算法用蒙特卡洛方法估计梯度\n",
    "* 方差控制和样本效率是进一步研究方向\n",
    "\n",
    "\n"
   ],
   "id": "b96ab72edc11566c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "500309bedc4671ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1adc1cfc048640"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
