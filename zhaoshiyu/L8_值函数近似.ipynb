{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "该课讲解在状态空间过大或连续时，如何使用函数逼近方法进行值函数估计，并重点介绍经典的深度强化学习算法DQN。\n",
    "\n",
    "---\n",
    "\n",
    "# 第8课：值函数近似（Function Approximation for Value Functions）\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：例子 — 曲线拟合\n",
    "\n",
    "在现实强化学习中，状态空间常常非常大甚至连续，不能用表格（Tabular）方式存储每个状态的值。\n",
    "\n",
    "举例：\n",
    "\n",
    "* 给定一组离散样本点 $(x_i, y_i)$，拟合函数 $f(x; \\theta)$ 来近似 $y$\n",
    "* 类比强化学习中，用参数化函数 $V(s; \\theta)$ 近似状态值函数\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：原理 — 目标函数介绍\n",
    "\n",
    "目标是最小化估计值与真实值之间的误差，比如平方误差：\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{s \\sim d^\\pi} \\left[ \\big( V^\\pi(s) - V(s; \\theta) \\big)^2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：原理 — 优化算法和函数选择\n",
    "\n",
    "* **函数形式**：线性函数、非线性函数（如神经网络）\n",
    "* **优化方法**：梯度下降或随机梯度下降\n",
    "* **目标函数的梯度**：\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E} \\left[ 2 \\big( V^\\pi(s) - V(s; \\theta) \\big) \\nabla_\\theta V(s; \\theta) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：原理 — 示例与分析\n",
    "\n",
    "* 由于真实 $V^\\pi(s)$ 通常未知，使用**TD目标**代替\n",
    "* 更新形式（基于 TD(0)）：\n",
    "\n",
    "$$\n",
    "\\delta_t = r_{t+1} + \\gamma V(s_{t+1}; \\theta) - V(s_t; \\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta V(s_t; \\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5：Sarsa 和 Q-learning 的函数逼近\n",
    "\n",
    "* 类似更新可用于动作值函数 $Q(s,a; \\theta)$\n",
    "* Sarsa更新：\n",
    "\n",
    "$$\n",
    "\\delta_t = r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}; \\theta) - Q(s_t, a_t; \\theta)\n",
    "$$\n",
    "\n",
    "* Q-learning更新：\n",
    "\n",
    "$$\n",
    "\\delta_t = r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta) - Q(s_t, a_t; \\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6：DQN — 基本原理\n",
    "\n",
    "Deep Q-Network（DQN）使用深度神经网络逼近 $Q$ 函数\n",
    "\n",
    "---\n",
    "\n",
    "### 关键点：\n",
    "\n",
    "* 网络参数 $\\theta$ 直接输出 $Q(s,a; \\theta)$\n",
    "* 目标值用固定的目标网络参数 $\\theta^-$ 计算：\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta^-)\n",
    "$$\n",
    "\n",
    "* 损失函数：\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{(s,a,r,s')} \\left[ \\big( y_t - Q(s,a; \\theta) \\big)^2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7：DQN — Experience Replay（经验回放）\n",
    "\n",
    "* 存储智能体的经历 $(s,a,r,s')$ 于回放缓冲区\n",
    "* 训练时从缓冲区随机采样小批量数据，打破数据相关性\n",
    "* 提升训练稳定性和样本利用率\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8：DQN — 代码与例子\n",
    "\n",
    "```python\n",
    "# 简要伪代码\n",
    "Initialize replay memory D\n",
    "Initialize Q-network with random weights θ\n",
    "Initialize target network weights θ- = θ\n",
    "\n",
    "for episode in range(M):\n",
    "    Initialize state s\n",
    "    for t in range(T):\n",
    "        Choose action a from s using ε-greedy policy derived from Q(s,a; θ)\n",
    "        Take action a, observe reward r and next state s'\n",
    "        Store transition (s,a,r,s') in D\n",
    "        Sample random minibatch from D\n",
    "        For each sample:\n",
    "            y = r + γ * max_a' Q(s', a'; θ-)\n",
    "        Update θ by minimizing (y - Q(s,a; θ))^2\n",
    "        Every C steps, update θ- = θ\n",
    "        s = s'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 本课总结\n",
    "\n",
    "* 函数逼近是解决大规模或连续状态空间的关键\n",
    "* TD目标和函数逼近结合，形成近似TD学习\n",
    "* DQN利用深度神经网络和经验回放，极大提升了Q-learning的表现\n",
    "\n"
   ],
   "id": "d4f3dd1664a003af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1dbd43ebe854f29b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ce17bc7eba3e2d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e700c684564c8879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9dc48981818553ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b64456e36f11fbcf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
