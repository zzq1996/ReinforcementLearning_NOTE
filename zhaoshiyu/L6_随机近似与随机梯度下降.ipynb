{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 第6课：随机近似与随机梯度下降\n",
    "\n",
    "> 关键词：Robbins-Monro、Stochastic Approximation、SGD、收敛性、梯度优化\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1：通过例子介绍 Iterative Mean Estimation（均值迭代估计）\n",
    "\n",
    "我们考虑一个基础问题：\n",
    "\n",
    "> 如何从一个**未知分布中抽样**，估计其数学期望 $\\mu = \\mathbb{E}[X]$？\n",
    "\n",
    "---\n",
    "\n",
    "### 方法1：批量平均\n",
    "\n",
    "若已获得 $n$ 个样本 $x_1, \\dots, x_n$，用\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "这就是普通的**样本均值**。\n",
    "\n",
    "---\n",
    "\n",
    "### 方法2：迭代均值更新\n",
    "\n",
    "如果数据是**流式（online）**到达，能否用**迭代方式更新估计值**？\n",
    "\n",
    "假设已有估计 $\\hat{\\mu}_n$，新来一个样本 $x_{n+1}$，则更新公式为：\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{n+1} = \\hat{\\mu}_n + \\alpha_n \\left( x_{n+1} - \\hat{\\mu}_n \\right)\n",
    "$$\n",
    "\n",
    "其中 $\\alpha_n = \\frac{1}{n+1}$，称为**学习率（step-size）**\n",
    "\n",
    "这个思想，就是 **Robbins-Monro 随机近似算法**的雏形。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：Robbins-Monro 算法介绍与例子\n",
    "\n",
    "### 问题定义（根的随机估计）：\n",
    "\n",
    "假设我们想解一个 **期望等式**：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[F(\\theta)] = 0\n",
    "$$\n",
    "\n",
    "但我们无法直接计算 $\\mathbb{E}$，只能观察带噪声样本 $F_n(\\theta)$，即：\n",
    "\n",
    "$$\n",
    "F_n(\\theta) = F(\\theta) + \\text{noise}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Robbins-Monro 迭代算法：\n",
    "\n",
    "$$\n",
    "\\theta_{n+1} = \\theta_n - \\alpha_n F_n(\\theta_n)\n",
    "$$\n",
    "\n",
    "* $\\alpha_n$：学习率序列，一般为 $\\alpha_n = \\frac{1}{n}$ 或 $\\frac{1}{n^k}, \\; 0.5 < k \\leq 1$\n",
    "\n",
    "---\n",
    "\n",
    "### 例子：迭代估计均值\n",
    "\n",
    "设 $F(\\theta) = \\theta - x$，我们希望找到 $\\theta$ 使 $\\mathbb{E}[\\theta - X] = 0$，即 $\\theta = \\mathbb{E}[X]$\n",
    "\n",
    "算法为：\n",
    "\n",
    "$$\n",
    "\\theta_{n+1} = \\theta_n - \\alpha_n (\\theta_n - x_n) = (1 - \\alpha_n)\\theta_n + \\alpha_n x_n\n",
    "$$\n",
    "\n",
    "这与 Part 1 的均值迭代完全一致。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3：Robbins-Monro 收敛性与应用\n",
    "\n",
    "### 收敛条件（Robbins-Monro 定理）：\n",
    "\n",
    "若：\n",
    "\n",
    "* $\\sum_n \\alpha_n = \\infty$\n",
    "* $\\sum_n \\alpha_n^2 < \\infty$\n",
    "\n",
    "则算法几乎必然收敛到真正的解（以概率1）。\n",
    "\n",
    "---\n",
    "\n",
    "### 常用步长设置：\n",
    "\n",
    "* $\\alpha_n = \\frac{1}{n}$：满足条件，收敛但较慢\n",
    "* $\\alpha_n = \\alpha$（固定步长）：不会完全收敛，但可以近似收敛（适用于非平稳环境）\n",
    "\n",
    "---\n",
    "\n",
    "### 应用场景：\n",
    "\n",
    "| 应用               | 场景说明              |\n",
    "| ---------------- | ----------------- |\n",
    "| TD(0)、Q-learning | 用估计值替代真值，进行增量式更新  |\n",
    "| 策略梯度法            | 用采样估计梯度，再用 SGD 更新 |\n",
    "| DQN              | 用目标网络估值，再更新当前网络参数 |\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4：随机梯度下降算法介绍（SGD）\n",
    "\n",
    "在监督学习或强化学习中，我们常面对优化目标：\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ \\ell(f_\\theta(x), y) \\right]\n",
    "$$\n",
    "\n",
    "但由于无法访问整个分布，**只能用样本进行梯度估计**。\n",
    "\n",
    "---\n",
    "\n",
    "### SGD 更新公式：\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla_\\theta \\ell(f_\\theta(x_t), y_t)\n",
    "$$\n",
    "\n",
    "每步使用一个样本或小批量进行更新，成本低、速度快、易在线学习。\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5：SGD 例子与收敛性\n",
    "\n",
    "### 例子：线性回归\n",
    "\n",
    "损失函数：\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\frac{1}{2} (\\theta^T x - y)^2\n",
    "$$\n",
    "\n",
    "随机样本更新：\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\alpha_t (\\theta^T x - y) x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 收敛性说明：\n",
    "\n",
    "若满足：\n",
    "\n",
    "* 学习率满足 Robbins-Monro 条件\n",
    "* 梯度方差有限\n",
    "\n",
    "则 SGD 在凸目标函数下会收敛到全局最优。\n",
    "\n",
    "---\n",
    "\n",
    "### 在深度强化学习中的意义：\n",
    "\n",
    "* DQN、Actor-Critic 中使用 SGD 更新参数化的值函数或策略\n",
    "* TD 学习使用类似于 SGD 的形式更新值函数\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6：SGD 的有趣性质\n",
    "\n",
    "| 性质          | 说明                                |\n",
    "| ----------- | --------------------------------- |\n",
    "| 噪声促进跳出局部极小  | 对非凸问题（如神经网络），SGD 的随机性有助于跳出鞍点或局部极小 |\n",
    "| 在线性问题中逼近最优解 | 收敛速度与特征协方差矩阵有关（见RMSProp、Adam改进）   |\n",
    "| 可结合动量、正则化   | 可提升收敛速度和泛化能力                      |\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7：SGD vs BGD vs MBGD 比较\n",
    "\n",
    "| 方法                  | 更新单位       | 速度 | 稳定性 | 内存 | 实时性    |\n",
    "| ------------------- | ---------- | -- | --- | -- | ------ |\n",
    "| BGD（Batch GD）       | 所有样本       | 慢  | 稳定  | 高  | 否      |\n",
    "| MBGD（Mini-Batch GD） | 一小批样本（如32） | 快  | 较稳定 | 适中 | 一般     |\n",
    "| SGD                 | 单个样本       | 极快 | 不稳定 | 最小 | 支持在线学习 |\n",
    "\n",
    "---\n",
    "\n",
    "### 总结推荐：\n",
    "\n",
    "| 场景            | 推荐方法     |\n",
    "| ------------- | -------- |\n",
    "| 模型已知，数据规模小    | BGD      |\n",
    "| 模型未知，数据中等     | MBGD     |\n",
    "| 流式数据、大规模、强化学习 | SGD（或变种） |\n",
    "\n",
    "---\n",
    "\n",
    "## 课后总结\n",
    "\n",
    "* **Robbins-Monro** 奠定了在线优化的基础，是 TD、策略优化的理论基石\n",
    "* **SGD** 是现代深度强化学习的优化核心\n",
    "* **收敛性**依赖于步长设置和梯度噪声控制\n",
    "* **在强化学习中，SGD 与策略值函数更新深度融合**\n",
    "\n"
   ],
   "id": "282c4767b784d2fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a8fd22607afa3ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "62ec54ba704181bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
